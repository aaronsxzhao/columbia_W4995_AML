{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvpjs15BAHTI"
   },
   "source": [
    "**Homework 4 Spring 202**\n",
    "\n",
    "**Due Date** - **11/23/2022**\n",
    "\n",
    "Your Name -\n",
    "\n",
    "Your UNI - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iODY3WiAx01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crpOuONCI49S"
   },
   "source": [
    "# PART 2 CIFAR 10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G_MvmKWI12F"
   },
   "source": [
    "CIFAR-10 is a dataset of 60,000 color images (32 by 32 resolution) across 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). The train/test split is 50k/10k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR6c_PPqh-_G",
    "outputId": "64fc264e-6df3-4db6-a3bb-3300d46272c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_dev, y_dev), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loYUlUPdh-_G"
   },
   "outputs": [],
   "source": [
    "LABELS = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPlBiR7yJ7Us"
   },
   "source": [
    "2.1 Plot 5 samples from each class/label from train set on a 10*5 subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4H4x-C0HCBS"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3e8jURBM4pp"
   },
   "source": [
    "2.2  Preparing the dataset for CNN \n",
    "\n",
    "1) Print the shapes - $x_{dev}, y_{dev},x_{test},y_{test}$\n",
    "\n",
    "2) Flatten the images into one-dimensional vectors and again print the shapes of $x_{dev}$,$x_{test}$\n",
    "\n",
    "3) Standardize the development and test sets.\n",
    "\n",
    "4) Train-test split your development set into train and validation sets (8:2 ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcxjQGQEHE1k"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfYuLxIoTNQ4"
   },
   "source": [
    "2.3 Build the feed forward network \n",
    "\n",
    "First hidden layer size - 128\n",
    "\n",
    "Second hidden layer size - 64\n",
    "\n",
    "Third and last layer size - You should know this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQn6Txl4HIIV"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llw83cB7UT4w"
   },
   "source": [
    "2.4) Print out the model summary. Can show show the calculation for each layer for estimating the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jmtUM2xHMC0"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9niV2Ozph-_I"
   },
   "source": [
    "2.5) Do you think this number is dependent on the image height and width? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FmAIfJSHPrN"
   },
   "outputs": [],
   "source": [
    "# Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxRLX68dUxKw"
   },
   "source": [
    "**Printing out your model's output on first train sample. This will confirm if your dimensions are correctly set up. The sum of this output equal to 1 upto two decimal places?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBX2x6DmVPcq",
    "outputId": "94554d92-da85-457b-c19d-ba3078dc9a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 179ms/step\n",
      "Output: 1.00\n"
     ]
    }
   ],
   "source": [
    "#modify name of X_train based on your requirement\n",
    "\n",
    "model.compile()\n",
    "output = model.predict(X_train[0].reshape(1,-1))\n",
    "\n",
    "# print(output)\n",
    "print(\"Output: {:.2f}\".format(sum(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQHWGj7KV31A"
   },
   "source": [
    "2.6) Using the right metric and  the right loss function, with Adam as the optimizer, train your model for 20 epochs with batch size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uw6vZJ1QHYs_"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTDEyTblWFRX"
   },
   "source": [
    "2.7) Plot a separate plots for:\n",
    "\n",
    "a. displaying train vs validation loss over each epoch\n",
    "\n",
    "b. displaying train vs validation accuracy over each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AegluUUHbNB"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58E6SCcxWa3F"
   },
   "source": [
    "2.8) Finally, report the metric chosen on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5QwNONvHdH7"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk4p3-YL1V8X"
   },
   "source": [
    "2.9 If the accuracy achieved is quite less(<50%), try improve the accuracy [Open ended question, you may try different approaches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0bKeyBwHegb"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THvjgBtRW8dd"
   },
   "source": [
    "2.10 Plot the first 50 samples of test dataset on a 10*5 subplot and this time label the images with both the ground truth (GT) and predicted class (P). (Make sure you predict the class with the improved model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXVtw7BhHfsP"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd0p6kbJadYz"
   },
   "source": [
    "# PART 3 Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvuOt3xdZChu"
   },
   "source": [
    "In this part of the homework, we will build and train a classical convolutional neural network on the CIFAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d900Q9tlmJWX",
    "outputId": "84f47da2-f10e-46a6-c10b-4133405080ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dev: (50000, 32, 32, 3),y_dev: (50000, 1),x_test: (10000, 32, 32, 3),y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_dev, y_dev), (x_test, y_test) = cifar10.load_data()\n",
    "print(\"x_dev: {},y_dev: {},x_test: {},y_test: {}\".format(x_dev.shape, y_dev.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "x_dev, x_test = x_dev.astype('float32'), x_test.astype('float32')\n",
    "x_dev = x_dev/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_dev, y_dev,test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6KCYFRUZKn2"
   },
   "source": [
    "3.1 We will be implementing the one of the first CNN models put forward by Yann LeCunn, which is commonly refered to as LeNet-5. The network has the following layers:\n",
    "\n",
    "1) 2D convolutional layer with 6 filters, 5x5 kernel, stride of 1 padded to yield the same size as input, ReLU activation\n",
    "\n",
    "2) Maxpooling layer of 2x2\n",
    "\n",
    "3) 2D convolutional layer with 16 filters, 5x5 kernel, 0 padding, ReLU activation\n",
    "\n",
    "4 )Maxpooling layer of 2x2\n",
    "\n",
    "5) 2D convolutional layer with 120 filters, 5x5 kernel, ReLU activation. Note that this layer has 120 output channels (filters), and each channel has only 1 number. The output of this layer is just a vector with 120 units!\n",
    "\n",
    "6) A fully connected layer with 84 units, ReLU activation\n",
    "\n",
    "7) The output layer where each unit respresents the probability of image being in that category. What activation function should you use in this layer? (You should know this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VmYqQiqHnF5"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1yQw2MSbJ8K"
   },
   "source": [
    "3.2 Report the model summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrCEa90vHpy4"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sBM5OlZbw64"
   },
   "source": [
    "3.3 Model Training\n",
    "\n",
    "1) Train the model for 20 epochs. In each epoch, record the loss and metric (chosen in part 3) scores for both train and validation sets.\n",
    "\n",
    "2) Plot a separate plots for:\n",
    "\n",
    "* displaying train vs validation loss over each epoch\n",
    "* displaying train vs validation accuracy over each epoch\n",
    "\n",
    "3) Report the model performance on the test set. Feel free to tune the hyperparameters such as batch size and optimizers to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9O-yNtFIGUf"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnUPRZXXcIbD"
   },
   "source": [
    "3.4 Overfitting\n",
    "\n",
    "1) To overcome overfitting, we will train the network again with dropout this time. For hidden layers use dropout probability of 0.3. Train the model again for 20 epochs. Report model performance on test set. \n",
    "\n",
    "Plot a separate plots for:\n",
    "\n",
    "*   displaying train vs validation loss over each epoch\n",
    "*   displaying train vs validation accuracy over each epoch \n",
    "\n",
    "2) This time, let's apply a batch normalization after every hidden layer, train the model for 20 epochs, report model performance on test set as above. \n",
    "\n",
    "Plot a separate plots for:\n",
    "\n",
    "*   displaying train vs validation loss over each epoch\n",
    "*   displaying train vs validation accuracy over each epoch \n",
    "\n",
    "3) Compare batch normalization technique with the original model and with dropout, which technique do you think helps with overfitting better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrz4xsNptsnt"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
